{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_rate = 0.4\n",
    "# warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from keras.preprocessing import text, sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ildoonet/pytorch-gradual-warmup-lr/blob/master/warmup_scheduler/scheduler.py\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class GradualWarmupScheduler(_LRScheduler):\n",
    "    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n",
    "    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        multiplier: target learning rate = base lr * multiplier\n",
    "        total_epoch: target learning rate is reached at total_epoch, gradually\n",
    "        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        if self.multiplier <= 1.:\n",
    "            raise ValueError('multiplier should be greater than 1.')\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "\n",
    "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
    "        if self.last_epoch <= self.total_epoch:\n",
    "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            if epoch is None:\n",
    "                self.after_scheduler.step(metrics, None)\n",
    "            else:\n",
    "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
    "\n",
    "    def step(self, epoch=None, metrics=None):\n",
    "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
    "            if self.finished and self.after_scheduler:\n",
    "                if epoch is None:\n",
    "                    self.after_scheduler.step(None)\n",
    "                else:\n",
    "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
    "            else:\n",
    "                return super(GradualWarmupScheduler, self).step(epoch)\n",
    "        else:\n",
    "            self.step_ReduceLROnPlateau(metrics, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    # handler1\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(asctime)s %(levelname)8s %(message)s\"))\n",
    "    # handler2\n",
    "    handler2 = FileHandler(filename=datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")+\".log\")\n",
    "    handler2.setFormatter(Formatter(\"%(asctime)s %(levelname)8s %(message)s\"))\n",
    "    # addHandler\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log_loss(y_true, y_pred, epsilon=1e-12):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "    return -(np.log(y_pred) * y_true + np.log(1 - y_pred) * (1 - y_true))\n",
    "\n",
    "class JigsawEvaluator:\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = (y_true >= 0.5).astype(int)\n",
    "        self.y_i = (y_identity >= 0.5).astype(int)\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score, bias_metrics[0], bias_metrics[1], bias_metrics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger\n",
    "logger = get_logger()\n",
    "\n",
    "# parameters\n",
    "n_workers = 4\n",
    "n_splits = 5\n",
    "seed = 777\n",
    "seed_everything(seed)\n",
    "\n",
    "maxlen = 300\n",
    "max_features = 410047\n",
    "\n",
    "batch_size = 512\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "n_fold = 4\n",
    "drop_rate = 0.4 # default=0.3\n",
    "\n",
    "# path\n",
    "CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\n",
    "GLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n",
    "# GOOGLE_EMBEDDING_PATH = '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "# WIKI_EMBEDDING_PATH = '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "\n",
    "# constants\n",
    "target = 'target'\n",
    "aux_target = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (1804874, 45)\n",
      "test shape: (97320, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "print(f'train shape: {train.shape}')\n",
    "print(f'test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD_PATH = \"../input/toxic-folds/fold01.csv\"\n",
    "fold = pd.read_csv(FOLD_PATH)\n",
    "train[\"fold_id\"] = fold[\"fold_id\"].values\n",
    "del fold; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD_PATH_JIGSAW = \"../input/jigsaw-old-folds/folds_oldcompe.csv\"\n",
    "OLD_PATH = \"../input/jigsaw-toxic-comment-classification-challenge/train.csv\"\n",
    "\n",
    "old_folds = pd.read_csv(FOLD_PATH_JIGSAW)\n",
    "old_df = pd.read_csv(OLD_PATH)\n",
    "old_df[\"target\"] = old_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(axis=1)\n",
    "old_df[\"target\"] = (old_df[\"target\"] >= 1).astype(\"int8\")\n",
    "old_df[\"fold_id\"] = old_folds.fold_id\n",
    "old_df = old_df[old_folds.fold_id != n_fold]\n",
    "old_y = np.where(old_df['target'] >= 0.5, 1, 0)\n",
    "del old_folds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text symbols\n",
    "symbols_to_isolate = '.,?!-;*\"‚Ä¶:‚Äî()%#$&_/@Ôºº„Éªœâ+=‚Äù‚Äú[]^‚Äì>\\\\¬∞<~‚Ä¢‚â†‚Ñ¢Àà ä…í‚àû¬ß{}¬∑œÑŒ±‚ù§‚ò∫…°|¬¢‚ÜíÃ∂`‚ù•‚îÅ‚î£‚î´‚îóÔºØ‚ñ∫‚òÖ¬©‚Äï…™‚úî¬Æ\\x96\\x92‚óè¬£‚ô•‚û§¬¥¬π‚òï‚âà√∑‚ô°‚óê‚ïë‚ñ¨‚Ä≤…îÀê‚Ç¨€©€û‚Ä†Œº‚úí‚û•‚ïê‚òÜÀå‚óÑ¬Ω ªœÄŒ¥Œ∑ŒªœÉŒµœÅŒΩ É‚ú¨Ôº≥ÔºµÔº∞Ôº•Ôº≤Ôº©Ôº¥‚òª¬±‚ôç¬µ¬∫¬æ‚úì‚óæÿüÔºé‚¨Ö‚ÑÖ¬ª–í–∞–≤‚ù£‚ãÖ¬ø¬¨‚ô´Ôº£Ôº≠Œ≤‚ñà‚ñì‚ñí‚ñë‚áí‚≠ê‚Ä∫¬°‚ÇÇ‚ÇÉ‚ùß‚ñ∞‚ñî‚óû‚ñÄ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ÜôŒ≥ÃÑ‚Ä≥‚òπ‚û°¬´œÜ‚Öì‚Äû‚úãÔºö¬•Ã≤ÃÖÃÅ‚àô‚Äõ‚óá‚úè‚ñ∑‚ùì‚ùó¬∂ÀöÀôÔºâ—Å–∏ ø‚ú®„ÄÇ…ë\\x80‚óïÔºÅÔºÖ¬Ø‚àíÔ¨ÇÔ¨Å‚ÇÅ¬≤ å¬º‚Å¥‚ÅÑ‚ÇÑ‚å†‚ô≠‚úò‚ï™‚ñ∂‚ò≠‚ú≠‚ô™‚òî‚ò†‚ôÇ‚òÉ‚òé‚úà‚úå‚ú∞‚ùÜ‚òô‚óã‚Ä£‚öìÂπ¥‚àé‚Ñí‚ñ™‚ñô‚òè‚ÖõÔΩÉÔΩÅÔΩì«Ä‚ÑÆ¬∏ÔΩó‚Äö‚àº‚Äñ‚Ñ≥‚ùÑ‚Üê‚òº‚ãÜ í‚äÇ„ÄÅ‚Öî¬®Õ°‡πè‚öæ‚öΩŒ¶√óŒ∏Ôø¶ÔºüÔºà‚ÑÉ‚è©‚òÆ‚ö†Êúà‚úä‚ùå‚≠ï‚ñ∏‚ñ†‚áå‚òê‚òë‚ö°‚òÑ«´‚ï≠‚à©‚ïÆÔºå‰æãÔºû ï…êÃ£Œî‚ÇÄ‚úû‚îà‚ï±‚ï≤‚ñè‚ñï‚îÉ‚ï∞‚ñä‚ñã‚ïØ‚î≥‚îä‚â•‚òí‚Üë‚òù…π‚úÖ‚òõ‚ô©‚òûÔº°Ôº™Ôº¢‚óî‚ó°‚Üì‚ôÄ‚¨ÜÃ±‚Ñè\\x91‚†ÄÀ§‚ïö‚Ü∫‚á§‚àè‚úæ‚ó¶‚ô¨¬≥„ÅÆÔΩúÔºè‚àµ‚à¥‚àöŒ©¬§‚òú‚ñ≤‚Ü≥‚ñ´‚Äø‚¨á‚úßÔΩèÔΩñÔΩçÔºçÔºíÔºêÔºòÔºá‚Ä∞‚â§‚àïÀÜ‚öú‚òÅ'\n",
    "symbols_to_delete = '\\nüçï\\rüêµüòë\\xa0\\ue014\\t\\uf818\\uf04a\\xadüò¢üê∂Ô∏è\\uf0e0üòúüòéüëä\\u200b\\u200eüòÅÿπÿØŸàŸäŸáÿµŸÇÿ£ŸÜÿßÿÆŸÑŸâÿ®ŸÖÿ∫ÿ±üòçüíñüíµ–ïüëéüòÄüòÇ\\u202a\\u202cüî•üòÑüèªüí•·¥ç è Ä·¥á…¥·¥Ö·¥è·¥Ä·¥ã ú·¥ú ü·¥õ·¥Ñ·¥ò ô“ì·¥ä·¥°…¢üòãüëè◊©◊ú◊ï◊ù◊ë◊ôüò±‚Äº\\x81„Ç®„É≥„Ç∏ÊïÖÈöú\\u2009üöå·¥µÕûüåüüòäüò≥üòßüôÄüòêüòï\\u200füëçüòÆüòÉüòò◊ê◊¢◊õ◊óüí©üíØ‚õΩüöÑüèº‡Æúüòñ·¥†üö≤‚Äêüòüüòàüí™üôèüéØüåπüòáüíîüò°\\x7füëå·ºê·Ω∂ŒÆŒπ·Ω≤Œ∫·ºÄŒØ·øÉ·º¥ŒæüôÑÔº®üò†\\ufeff\\u2028üòâüò§‚õ∫üôÇ\\u3000ÿ™ÿ≠ŸÉÿ≥ÿ©üëÆüíôŸÅÿ≤ÿ∑üòèüçæüéâüòû\\u2008üèæüòÖüò≠üëªüò•üòîüòìüèΩüéÜüçªüçΩüé∂üå∫ü§îüò™\\x08‚Äëüê∞üêáüê±üôÜüò®üôÉüíïùòäùò¶ùò≥ùò¢ùòµùò∞ùò§ùò∫ùò¥ùò™ùòßùòÆùò£üíóüíöÂú∞ÁçÑË∞∑—É–ª–∫–Ω–ü–æ–ê–ùüêæüêïüòÜ◊îüîóüöΩÊ≠åËàû‰ºéüôàüò¥üèøü§óüá∫üá∏–ºœÖ—Ç—ï‚§µüèÜüéÉüò©\\u200aüå†üêüüí´üí∞üíé—ç–ø—Ä–¥\\x95üñêüôÖ‚õ≤üç∞ü§êüëÜüôå\\u2002üíõüôÅüëÄüôäüôâ\\u2004À¢·µí ≥ ∏·¥º·¥∑·¥∫ ∑·µó ∞·µâ·µò\\x13üö¨ü§ì\\ue602üòµŒ¨ŒøœåœÇŒ≠·Ω∏◊™◊û◊ì◊£◊†◊®◊ö◊¶◊òüòíÕùüÜïüëÖüë•üëÑüîÑüî§üëâüë§üë∂üë≤üîõüéì\\uf0b7\\uf04c\\x9f\\x10ÊàêÈÉΩüò£‚è∫üòåü§ëüåèüòØ–µ—Öüò≤·º∏·æ∂·ΩÅüíûüöìüîîüìöüèÄüëê\\u202düí§üçá\\ue613Â∞èÂúüË±Üüè°‚ùî‚Åâ\\u202füë†„Äã‡§ï‡§∞‡•ç‡§Æ‡§æüáπüáºüå∏Ëî°Ëã±Êñáüåûüé≤„É¨„ÇØ„Çµ„ÇπüòõÂ§ñÂõΩ‰∫∫ÂÖ≥Á≥ª–°–±üíãüíÄüéÑüíúü§¢ŸêŸé—å—ã–≥—è‰∏çÊòØ\\x9c\\x9düóë\\u2005üíÉüì£üëø‡ºº„Å§‡ºΩüò∞·∏∑–ó–∑‚ñ±—ÜÔøºü§£ÂçñÊ∏©Âì•ÂçéËÆÆ‰ºö‰∏ãÈôç‰Ω†Â§±ÂéªÊâÄÊúâÁöÑÈí±Âä†ÊãøÂ§ßÂùèÁ®éÈ™óÂ≠êüêù„ÉÑüéÖ\\x85üç∫ÿ¢ÿ•ÿ¥ÿ°üéµüåéÕü·ºîÊ≤πÂà´ÂÖãü§°ü§•üò¨ü§ß–π\\u2003üöÄü§¥ ≤—à—á–ò–û–†–§–î–Ø–ú—é–∂üòùüñë·Ωê·ΩªœçÁâπÊÆä‰ΩúÊà¶Áæ§—âüí®ÂúÜÊòéÂõ≠◊ß‚Ñêüèàüò∫üåç‚èè·ªáüçîüêÆüçÅüçÜüçëüåÆüåØü§¶\\u200dùìíùì≤ùìøùìµÏïàÏòÅÌïòÏÑ∏Ïöî–ñ—ô–ö—õüçÄüò´ü§§·ø¶ÊàëÂá∫ÁîüÂú®‰∫ÜÂèØ‰ª•ËØ¥ÊôÆÈÄöËØùÊ±âËØ≠Â•ΩÊûÅüéºüï∫üç∏ü•ÇüóΩüéáüéäüÜòü§†üë©üñíüö™Â§©‰∏ÄÂÆ∂‚ö≤\\u2006‚ö≠‚öÜ‚¨≠‚¨Ø‚èñÊñ∞‚úÄ‚ïåüá´üá∑üá©üá™üáÆüá¨üáßüò∑üá®üá¶–•–®üåê\\x1fÊùÄÈ∏°ÁªôÁå¥Áúã Åùó™ùóµùó≤ùóªùòÜùóºùòÇùóøùóÆùóπùó∂ùòáùóØùòÅùó∞ùòÄùòÖùóΩùòÑùó±üì∫œñ\\u2000“Ø’Ω·¥¶·é•“ªÕ∫\\u2007’∞\\u2001…©ÔΩôÔΩÖ‡µ¶ÔΩå∆ΩÔΩàùêìùê°ùêûùê´ùêÆùêùùêöùêÉùêúùê©ùê≠ùê¢ùê®ùêß∆Ñ·¥®◊ü·ëØ‡ªêŒ§·èß‡Ø¶–Ü·¥ë‹Åùê¨ùê∞ùê≤ùêõùê¶ùêØùêëùêôùê£ùêáùêÇùêòùüé‘ú–¢·óû‡±¶„Äî·é´ùê≥ùêîùê±ùüîùüìùêÖüêãÔ¨Éüíòüíì—ëùò•ùòØùò∂üíêüåãüåÑüåÖùô¨ùôñùô®ùô§ùô£ùô°ùôÆùôòùô†ùôöùôôùôúùôßùô•ùô©ùô™ùôóùôûùôùùôõüë∫üê∑‚ÑãùêÄùê•ùê™üö∂ùô¢·ºπü§òÕ¶üí∏ÿ¨Ìå®Ìã∞Ôº∑ùôá·µªüëÇüëÉ…úüé´\\uf0a7–ë–£—ñüö¢üöÇ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä·øÜüèÉùì¨ùìªùì¥ùìÆùìΩùìº‚òòÔ¥æÃØÔ¥ø‚ÇΩ\\ue807ùëªùíÜùíçùíïùíâùíìùíñùíÇùíèùíÖùíîùíéùíóùíäüëΩüòô\\u200c–õ‚Äíüéæüëπ‚éåüèí‚õ∏ÂÖ¨ÂØìÂÖªÂÆ†Áâ©ÂêóüèÑüêÄüöëü§∑ÊìçÁæéùíëùíöùíêùë¥ü§ôüêíÊ¨¢ËøéÊù•Âà∞ÈòøÊãâÊñØ◊°◊§ùô´üêàùíåùôäùô≠ùôÜùôãùôçùòºùôÖÔ∑ªü¶ÑÂ∑®Êî∂Ëµ¢ÂæóÁôΩÈ¨ºÊÑ§ÊÄíË¶Å‰π∞È¢ù·∫Ωüöóüê≥ùüèùêüùüñùüëùüïùíÑùüóùê†ùôÑùôÉüëáÈîüÊñ§Êã∑ùó¢ùü≥ùü±ùü¨‚¶Å„Éû„É´„Éè„Éã„ÉÅ„É≠Ê†™ÂºèÁ§æ‚õ∑ÌïúÍµ≠Ïñ¥„Ñ∏„ÖìÎãàÕú ñùòøùôî‚Çµùí©‚ÑØùíæùìÅùí∂ùìâùìáùìäùìÉùìàùìÖ‚Ñ¥ùíªùíΩùìÄùìåùí∏ùìéùôèŒ∂ùôüùòÉùó∫ùüÆùü≠ùüØùü≤üëãü¶äÂ§ö‰º¶üêΩüéªüéπ‚õìüèπüç∑ü¶Ü‰∏∫Âíå‰∏≠ÂèãË∞äÁ•ùË¥∫‰∏éÂÖ∂ÊÉ≥Ë±°ÂØπÊ≥ïÂ¶ÇÁõ¥Êé•ÈóÆÁî®Ëá™Â∑±ÁåúÊú¨‰º†ÊïôÂ£´Ê≤°ÁßØÂîØËÆ§ËØÜÂü∫Áù£ÂæíÊõæÁªèËÆ©Áõ∏‰ø°ËÄ∂Á®£Â§çÊ¥ªÊ≠ªÊÄ™‰ªñ‰ΩÜÂΩì‰ª¨ËÅä‰∫õÊîøÊ≤ªÈ¢òÊó∂ÂÄôÊàòËÉúÂõ†Âú£ÊääÂÖ®Â†ÇÁªìÂ©öÂ≠©ÊÅêÊÉß‰∏îÊ†óË∞ìËøôÊ†∑Ëøò‚ôæüé∏ü§ïü§í‚õëüéÅÊâπÂà§Ê£ÄËÆ®üèùü¶Åüôãüò∂Ï•êÏä§ÌÉ±Ìä∏Î§ºÎèÑÏÑùÏú†Í∞ÄÍ≤©Ïù∏ÏÉÅÏù¥Í≤ΩÏ†úÌô©ÏùÑÎ†µÍ≤åÎßåÎì§ÏßÄÏïäÎ°ùÏûòÍ¥ÄÎ¶¨Ìï¥ÏïºÌï©Îã§Ï∫êÎÇòÏóêÏÑúÎåÄÎßàÏ¥àÏôÄÌôîÏïΩÍ∏àÏùòÌíàÎü∞ÏÑ±Î∂ÑÍ∞àÎïåÎäîÎ∞òÎìúÏãúÌóàÎêúÏÇ¨Ïö©üî´üëÅÂá∏·Ω∞üí≤üóØùôà·ºåùíáùíàùíòùíÉùë¨ùë∂ùïæùñôùñóùñÜùñéùñåùñçùñïùñäùñîùñëùñâùñìùñêùñúùñûùñöùñáùïøùñòùñÑùñõùñíùñãùñÇùï¥ùñüùñàùï∏üëëüöøüí°Áü•ÂΩºÁôæ\\uf005ùôÄùíõùë≤ùë≥ùëæùíãùüíüò¶ùôíùòæùòΩüèêùò©ùò®·Ωº·πëùë±ùëπùë´ùëµùë™üá∞üáµüëæ·ìá·íß·î≠·êÉ·êß·ê¶·ë≥·ê®·ìÉ·ìÇ·ë≤·ê∏·ë≠·ëé·ìÄ·ê£üêÑüéàüî®üêéü§ûüê∏üíüüé∞üåùüõ≥ÁÇπÂáªÊü•Áâàüç≠ùë•ùë¶ùëßÔºÆÔºßüë£\\uf020„Å£üèâ—Ñüí≠üé•Œûüê¥üë®ü§≥ü¶ç\\x0büç©ùëØùííüòóùüêüèÇüë≥üçóüïâüê≤⁄Ü€åùëÆùóïùó¥üçíÍú•‚≤£‚≤èüêë‚è∞ÈâÑ„É™‰∫ã‰ª∂—óüíä„Äå„Äç\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ÁáªË£Ω„Ç∑ËôöÂÅΩÂ±ÅÁêÜÂ±à–ìùë©ùë∞ùíÄùë∫üå§ùó≥ùóúùóôùó¶ùóßüçä·Ω∫·ºà·º°œá·øñŒõ‚§èüá≥ùíôœà’Å’¥’•’º’°’µ’´’∂÷Ä÷Ç’§’±ÂÜ¨Ëá≥·ΩÄùíÅüîπü§öüçéùë∑üêÇüíÖùò¨ùò±ùò∏ùò∑ùòêùò≠ùòìùòñùòπùò≤ùò´⁄©Œíœéüí¢ŒúŒüŒùŒëŒïüá±‚ô≤ùùà‚Ü¥üíí‚äò»ªüö¥üñïüñ§ü•òüìçüëà‚ûïüö´üé®üåëüêªùêéùêçùêäùë≠ü§ñüééüòºüï∑ÔΩáÔΩíÔΩéÔΩîÔΩâÔΩÑÔΩïÔΩÜÔΩÇÔΩãùü∞üá¥üá≠üáªüá≤ùóûùó≠ùóòùó§üëºüìâüçüüç¶üåàüî≠„Ääüêäüêç\\uf10a·Éö⁄°üê¶\\U0001f92f\\U0001f92aüê°üí≥·º±üôáùó∏ùóüùó†ùó∑ü•ú„Åï„Çà„ÅÜ„Å™„Çâüîº'\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"‚Äú‚Äù‚Äô' + '‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî‚Äì&'\n",
    "small_caps_mapping = { \n",
    "\"·¥Ä\": \"a\", \" ô\": \"b\", \"·¥Ñ\": \"c\", \"·¥Ö\": \"d\", \"·¥á\": \"e\", \"“ì\": \"f\", \"…¢\": \"g\", \" ú\": \"h\", \"…™\": \"i\", \n",
    "\"·¥ä\": \"j\", \"·¥ã\": \"k\", \" ü\": \"l\", \"·¥ç\": \"m\", \"…¥\": \"n\", \"·¥è\": \"o\", \"·¥ò\": \"p\", \"«´\": \"q\", \" Ä\": \"r\", \n",
    "\"s\": \"s\", \"·¥õ\": \"t\", \"·¥ú\": \"u\", \"·¥†\": \"v\", \"·¥°\": \"w\", \"x\": \"x\", \" è\": \"y\", \"·¥¢\": \"z\"\n",
    "}\n",
    "contraction_mapping = {\n",
    "\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n",
    "\"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "\"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "\"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\":\"this is\",\"that'd\": \"that would\", \n",
    "\"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "\"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "\"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "\"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "\"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "\"trump's\": \"trump is\", \"obama's\": \"obama is\", \"canada's\": \"canada is\", \"today's\": \"today is\"\n",
    "}\n",
    "specail_signs = { \"‚Ä¶\": \"...\", \"‚ÇÇ\": \"2\"}\n",
    "specials = [\"‚Äô\", \"‚Äò\", \"¬¥\", \"`\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = treebank_tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def clean_text(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x\n",
    "\n",
    "def apply_clean_text(X):\n",
    "    if not isinstance(X, pd.Series):\n",
    "        X = pd.Series(X)\n",
    "    return X.apply(clean_text)\n",
    "    \n",
    "def parallel_clean_text(X):\n",
    "    with multiprocessing.Pool(processes=n_workers) as p:\n",
    "        splits = np.array_split(X, n_workers)\n",
    "        pool_results = p.map(apply_clean_text, splits)\n",
    "    return np.concatenate(pool_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_old = parallel_clean_text(old_df['comment_text'])\n",
    "del treebank_tokenizer, old_df['comment_text']; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_old):  127657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = pd.read_pickle('../input/toxicpreprocesseddata/tokenizer.pkl')\n",
    "X_old = tokenizer.texts_to_sequences(X_old)\n",
    "old_lengths = np.array([len(x) for x in X_old])\n",
    "X_old = sequence.pad_sequences(X_old, maxlen=maxlen)\n",
    "print(\"len(X_old): \", len(X_old))\n",
    "del tokenizer; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('../input/toxicpreprocesseddata/X_train.pkl')\n",
    "X_test = pd.read_pickle('../input/toxicpreprocesseddata/X_test.pkl')\n",
    "embedding_matrix = pd.read_pickle('../input/toxicpreprocesseddata/embedding_matrix.pkl')\n",
    "tokenizer = pd.read_pickle('../input/toxicpreprocesseddata/tokenizer.pkl')\n",
    "train_lengths = pd.read_pickle('../input/toxicpreprocesseddata/train_lengths.pkl')\n",
    "test_lengths = pd.read_pickle('../input/toxicpreprocesseddata/test_lengths.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 46)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created X_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created train\n",
      "created train_index, valid_index\n"
     ]
    }
   ],
   "source": [
    "# concatÂá¶ÁêÜ\n",
    "X_train = np.append(X_train, X_old, axis=0)\n",
    "print(\"created X_train\")\n",
    "train = train.append(old_df).reset_index(drop=True)\n",
    "print(\"created train\")\n",
    "train_lengths = np.append(train_lengths, old_lengths)\n",
    "del X_old, old_df, old_lengths; gc.collect()\n",
    "\n",
    "train_index = train[train.fold_id!=n_fold].index\n",
    "valid_index = train[train.fold_id==n_fold].index\n",
    "print(\"created train_index, valid_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360974"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(fixed valid_index(only original train data)):  360974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_index = [i for i in valid_index if i<1804874]\n",
    "print(\"len(fixed valid_index(only original train data)): \", len(valid_index))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360974"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "y shape: (1932531, 8)\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "weights = np.ones((len(train),)) / 4\n",
    "\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (((train[target].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (1-(train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int)) ) > 1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (((train[target].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "#loss_weight = 1.0 / weights.mean()\n",
    "loss_weight = 0.5\n",
    "print(loss_weight)\n",
    "\n",
    "y_train = np.vstack([train[target], weights]).T\n",
    "# y_train = np.vstack([np.where(train[target]>=0.5, train[target], 0), weights]).T\n",
    "y_aux_train = train[[target]+aux_target]\n",
    "y_train = np.hstack([y_train, y_aux_train])\n",
    "print(f'y shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch\n",
    "    \n",
    "def prepare_data_loader(X, lengths, y=None, shuffle=False):\n",
    "    if y is None:\n",
    "        dataset = TensorDataset(torch.from_numpy(X), \n",
    "                                torch.from_numpy(lengths))\n",
    "        collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n",
    "                                          sequence_index=0, \n",
    "                                          length_index=1)\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X), \n",
    "                                torch.from_numpy(lengths), \n",
    "                                torch.tensor(y, dtype=torch.float32))\n",
    "        collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n",
    "                                          sequence_index=0, \n",
    "                                          length_index=1, \n",
    "                                          label_index=2)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(data, targets):\n",
    "    ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:, 1:2])(data[:, :1], targets[:, :1])\n",
    "    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:, 1:], targets[:, 2:])\n",
    "    return (bce_loss_1 * loss_weight) + bce_loss_2\n",
    "\n",
    "#def custom_loss(data, targets):\n",
    "#    ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "#    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:, 1:2])(data[:, :1], targets[:, :1])\n",
    "#    return bce_loss_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class EmbLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_features, num_aux_targets=6):\n",
    "        super().__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(drop_rate)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(128 * 2, 128, bidirectional=True, batch_first=True)\n",
    "        #self.lstm2 = nn.GRU(128 * 2, 128, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(512, 512)\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        #self.linear1 = nn.Sequential(\n",
    "        #    nn.BatchNorm1d(512),\n",
    "        #    nn.Linear(512, 512),\n",
    "        #    #nn.PReLU(),\n",
    "        #    nn.ReLU(inplace=True),\n",
    "        #)\n",
    "        #self.linear2 = nn.Sequential(\n",
    "        #    nn.BatchNorm1d(512),\n",
    "        #    nn.Linear(512, 512),\n",
    "        #    #nn.PReLU(),\n",
    "        #    nn.ReLU(inplace=True),\n",
    "        #)\n",
    "        \n",
    "        self.linear_out = nn.Linear(512, 1)\n",
    "        self.linear_aux_out = nn.Linear(512, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        \n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class EmbLSTMGRUCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_features, num_aux_targets=6):\n",
    "        super().__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(drop_rate)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, 80, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.GRU(80*2, 80, bidirectional=True, batch_first=True)\n",
    "        self.cnn = nn.Conv1d(80*2, 64, kernel_size=3, padding=0)\n",
    "    \n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(458),\n",
    "            nn.Linear(458, 458),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(458),\n",
    "            nn.Linear(458, 458),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.linear_out = nn.Linear(458, 1)\n",
    "        self.linear_aux_out = nn.Linear(458, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        x = self.cnn(h_lstm2.permute(0, 2, 1))\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(x, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(x, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = self.linear1(h_conc)\n",
    "        h_conc_linear2  = self.linear2(h_conc)\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filepath, model, optimizer, epoch):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "def plot_losses(train_losses, valid_losses, fold=0):\n",
    "    plt.clf()\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(valid_losses, label='valid')\n",
    "    plt.legend()\n",
    "    plt.title(f'loss history of fold {fold}')\n",
    "    plt.savefig(f'loss_history_of_fold_{fold}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_histogram(y_true, y_pred, n_bins=50, fold=0):\n",
    "    bins = np.linspace(0, 1, n_bins)\n",
    "    plt.clf()\n",
    "    plt.hist(y_pred[:, 0], bins=bins, label='pred')\n",
    "    plt.hist(y_true[:, 0], bins=bins, label='true')\n",
    "    plt.legend()\n",
    "    plt.title(f'validation histogram of fold {fold}')\n",
    "    plt.savefig(f'validation_histogram_of_fold_{fold}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_aucs(aucs, auc_type, fold=0):\n",
    "    total = sum(np.power(aucs, -5))\n",
    "    score = np.power(total / len(aucs), 1 / -5)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    ax = sns.barplot(identity_columns, aucs)\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.4f}', (p.get_x()+p.get_width()/2, int(p.get_height()*0.95)),\n",
    "                    ha='center', va='center', fontsize=20, color='blue', xytext=(0, 20), \n",
    "                    textcoords='offset points')\n",
    "    plt.xticks(rotation=10)\n",
    "    plt.title(f'{auc_type} {score} barplot of fold {fold}')\n",
    "    plt.savefig(f'{auc_type}_{score}_barplot_of_fold_{fold}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, X):\n",
    "    logits = model(X)\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    return logits, probabilities\n",
    "        \n",
    "def evaluate_single_epoch(model, dataloader, criterion, epoch, evaluator, return_pred=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_preds = []\n",
    "        loss_list = []\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            X = X[0].cuda().long() # X[0]: text sequences, X[1]: lengths\n",
    "            y = y.cuda().float()\n",
    "            logits, probabilities = inference(model, X)\n",
    "            \n",
    "            loss = criterion(logits, y)\n",
    "            loss_list.append(loss.item())\n",
    "            valid_preds.append(probabilities.cpu().numpy())\n",
    "\n",
    "        valid_preds = np.concatenate(valid_preds)\n",
    "\n",
    "        log_dict = {}\n",
    "        score, subgroup_auc, bpsn_auc, bnsp_auc = evaluator.get_final_metric(valid_preds[:, 0])\n",
    "        log_dict['score'] = score\n",
    "        log_dict['subgroup_auc'] = subgroup_auc\n",
    "        log_dict['bpsn_auc'] = bpsn_auc\n",
    "        log_dict['bnsp_auc'] = bnsp_auc\n",
    "        log_dict['loss'] = np.mean(loss_list)\n",
    "        \n",
    "        if return_pred:\n",
    "            log_dict['pred'] = valid_preds\n",
    "            \n",
    "    return log_dict\n",
    "\n",
    "def train_single_epoch(model, dataloader, criterion, optimizer, epoch, parent_bar, scheduler=None):\n",
    "    model.train()\n",
    "    log_dict = {}\n",
    "    log_dict['loss'] = 0\n",
    "    for X, y in progress_bar(dataloader, parent=parent_bar):\n",
    "        X = X[0].cuda().long() # X[0]: text sequences, X[1]: lengths\n",
    "        y = y.cuda().float()\n",
    "        logits, probabilities = inference(model, X)\n",
    "        \n",
    "        loss = criterion(logits, y)\n",
    "        log_dict['loss'] += loss.item() / len(dataloader)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        log_dict['lr'] = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "        \n",
    "    return log_dict\n",
    "\n",
    "def predict_model(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for i, X in enumerate(dataloader):\n",
    "            X = X[0].cuda().long() # X[0]: text sequences, X[1]: lengths\n",
    "            logits, probabilities = inference(model, X)\n",
    "            test_preds.append(probabilities.cpu().numpy())\n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = train.copy()\n",
    "# kfold['fold_id'] = 0\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n",
    "# for fold, (train_index, valid_index) in enumerate(cv.split(range(len(train)), np.where(train[target]>0.5, 1, 0))):\n",
    "#     kfold.loc[valid_index, 'fold_id'] = fold\n",
    "# kfold[['fold_id']].to_csv('fold01.csv', index=False)\n",
    "\n",
    "# del kfold; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "# for fold, (train_index, valid_index) in enumerate(cv.split(range(len(train)), np.where(train[target]>0.5, 1, 0))):\n",
    "#     if fold==n_fold:\n",
    "        \n",
    "fold = n_fold\n",
    "\n",
    "# data split\n",
    "y_train[np.isnan(y_train)] = 0\n",
    "X_trn, X_val = X_train[train_index], X_train[valid_index]\n",
    "y_trn, y_val = y_train[train_index], y_train[valid_index]\n",
    "\n",
    "trn_lengths, val_lengths = train_lengths[train_index], train_lengths[valid_index]\n",
    "train = train.fillna(0)\n",
    "y_val_target = train.loc[valid_index, target].values\n",
    "y_val_identity = train.loc[valid_index, identity_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.00% [1/10 11:43<1:45:33]\n",
       "    </div>\n",
       "    \n",
       "Epoch 1 - avg_train_loss: 0.1444  avg_val_loss: 0.1455  val_score: 0.928394<p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='317' class='' max='3070', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.33% [317/3070 01:07<09:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-22 10:22:29,723     INFO Epoch 1 - optimizer: [{'lr': 0.0011, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'initial_lr': 0.001, 'params': [140040770468384, 140040770933024, 140040770469680, 140040770467664, 140040770468888, 140040770469608, 140040770469824, 140040770467376, 140040770466224, 140040770466728, 140040770467160, 140040770465936, 140040770466512, 140040770466008, 140040770925912, 140040770925120, 140040770926704, 140040770929792, 140040770931736, 140040770931304, 140040770930224, 140040770930152, 140040770930080, 140040770930872, 140040770931952]}] - scheduler: {'multiplier': 1.2, 'total_epoch': 2, 'after_scheduler': <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f5d964baa20>, 'finished': False, 'base_lrs': [0.001], 'last_epoch': 1}\n",
      "2019-06-22 10:34:08,755     INFO Epoch 1 - avg_train_loss: 0.1444 avg_val_loss: 0.1455  val_score: 0.928394\n",
      "2019-06-22 10:34:13,479     INFO Epoch 2 - optimizer: [{'lr': 0.0012, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'initial_lr': 0.001, 'params': [140040770468384, 140040770933024, 140040770469680, 140040770467664, 140040770468888, 140040770469608, 140040770469824, 140040770467376, 140040770466224, 140040770466728, 140040770467160, 140040770465936, 140040770466512, 140040770466008, 140040770925912, 140040770925120, 140040770926704, 140040770929792, 140040770931736, 140040770931304, 140040770930224, 140040770930152, 140040770930080, 140040770930872, 140040770931952]}] - scheduler: {'multiplier': 1.2, 'total_epoch': 2, 'after_scheduler': <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f5d964baa20>, 'finished': False, 'base_lrs': [0.001], 'last_epoch': 2}\n"
     ]
    }
   ],
   "source": [
    "train_loader = prepare_data_loader(X_trn, trn_lengths, y=y_trn, shuffle=True)\n",
    "valid_loader = prepare_data_loader(X_val, val_lengths, y=y_val, shuffle=False)\n",
    "evaluator = JigsawEvaluator(y_val_target, y_val_identity)\n",
    "\n",
    "# model\n",
    "model = EmbLSTM(embedding_matrix, max_features).cuda()\n",
    "#model = EmbLSTMGRUCNN(embedding_matrix, max_features).cuda()\n",
    "criterion = custom_loss\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, amsgrad=False)\n",
    "scheduler_cosine = CosineAnnealingLR(optimizer, T_max=4, eta_min=1e-3)\n",
    "scheduler = GradualWarmupScheduler(optimizer, multiplier=1.2, total_epoch=2, after_scheduler=scheduler_cosine)\n",
    "#     scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "#     scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "# main loop\n",
    "best_epoch = -1\n",
    "best_score = 0.\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "mb = master_bar(range(epochs))\n",
    "for epoch in mb:\n",
    "    scheduler.step() # 2 epoch warmup, after that schedule as scheduler_cosine\n",
    "    logger.info(f'Epoch {epoch+1} - optimizer: {optimizer.state_dict()[\"param_groups\"]} - scheduler: {scheduler.state_dict()}')\n",
    "    log_dict_train = train_single_epoch(model, train_loader, criterion, optimizer, epoch, mb) # loss, lr\n",
    "    log_dict_valid = evaluate_single_epoch(model, valid_loader, criterion, epoch, evaluator) # loss, score\n",
    "    train_losses.append(log_dict_train['loss'])\n",
    "    valid_losses.append(log_dict_valid['loss'])\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        mb.write(f'Epoch {epoch+1} - avg_train_loss: {log_dict_train[\"loss\"]:.4f}  avg_val_loss: {log_dict_valid[\"loss\"]:.4f}  val_score: {log_dict_valid[\"score\"]:.6f}')\n",
    "        logger.info(f'Epoch {epoch+1} - avg_train_loss: {log_dict_train[\"loss\"]:.4f} avg_val_loss: {log_dict_valid[\"loss\"]:.4f}  val_score: {log_dict_valid[\"score\"]:.6f}')\n",
    "\n",
    "    if log_dict_valid[\"score\"] > best_score:\n",
    "        best_epoch = epoch + 1\n",
    "        best_score = log_dict_valid[\"score\"]\n",
    "        save_checkpoint(f'weight_best_fold_{fold}.pth', model, optimizer, epoch)\n",
    "        if epoch - best_epoch > 3:\n",
    "            break\n",
    "\n",
    "# load best\n",
    "state = torch.load(f'weight_best_fold_{fold}.pth')\n",
    "model.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "# save valid\n",
    "log_dict = evaluate_single_epoch(model, valid_loader, criterion, 0, evaluator, return_pred=True)\n",
    "pred_valid = log_dict['pred']\n",
    "np.save(f'pred_valid_fold_{fold}.npy', pred_valid)\n",
    "\n",
    "# evaluate\n",
    "score, subgroup_auc, bpsn_auc, bnsp_auc = evaluator.get_final_metric(pred_valid[:, 0])\n",
    "subgroup_auc = evaluator._power_mean(subgroup_auc)\n",
    "bpsn_auc = evaluator._power_mean(bpsn_auc)\n",
    "bnsp_auc = evaluator._power_mean(bnsp_auc)\n",
    "overall_auc = evaluator._calculate_overall_auc(pred_valid[:, 0])\n",
    "logger.info(f'metric: {score:.6f}, overall auc: {overall_auc:.6f}, subgroup_auc: {subgroup_auc:.6f}, bpsn_auc: {bpsn_auc:.6f}, bnsp_auc: {bnsp_auc:.6f}')\n",
    "\n",
    "# plot\n",
    "plot_losses(train_losses, valid_losses, fold=fold)\n",
    "plot_histogram(y_val, pred_valid, n_bins=50, fold=fold)\n",
    "plot_aucs(log_dict['subgroup_auc'], 'subgroup_auc', fold=fold)\n",
    "plot_aucs(log_dict['bpsn_auc'], 'bpsn_auc', fold=fold)\n",
    "plot_aucs(log_dict['bnsp_auc'], 'bnsp_auc', fold=fold)\n",
    "\n",
    "del model; gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190446</th>\n",
       "      <td>474051</td>\n",
       "      <td>are they surprised?  surprised?  read what little local history exists - you should be able to map out the \"routes\" of the old trollies.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011919</td>\n",
       "      <td>4.429653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860284</th>\n",
       "      <td>5172977</td>\n",
       "      <td>Between Dalton McGuinty and Kathleen Wynne\\nthey doubled the provincial debt to over $300 Billion\\nand she has the nerve to even think of\\nBasic income\\n.\\nshame\\nunethical\\nimmoral\\nsinful\\ninsane\\nincompetent</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>4.290715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9110</th>\n",
       "      <td>253508</td>\n",
       "      <td>I.d.i.o.t.s all.</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>4.207807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664253</th>\n",
       "      <td>6162037</td>\n",
       "      <td>what rubbish\\n\\nwhy not examine the role of media as the crucial explanatory variable\\n\\neverything kids learn outside of school is  influenced by it</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015990</td>\n",
       "      <td>4.135787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492735</th>\n",
       "      <td>5947182</td>\n",
       "      <td>Alceste, I'm assuming you take a size medium pussyh at?</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>4.077752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76512</th>\n",
       "      <td>336206</td>\n",
       "      <td>I misspelled expect in my prior comment</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>4.028418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240658</th>\n",
       "      <td>5630606</td>\n",
       "      <td>Who gives a flying freak about what Kim and Park are making or not making? Really? This is what the SA chooses to write editorials about?</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>3.865027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960637</th>\n",
       "      <td>5293418</td>\n",
       "      <td>yes, stupiditing</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>3.781680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973847</th>\n",
       "      <td>5309299</td>\n",
       "      <td>try again loser\\nhttps://www.youtube.com/watch?v=SXxHfb66ZgM\\nhttps://www.youtube.com/watch?v=Zm7_FVS3IMo\\nhttps://www.youtube.com/watch?v=veLJSKXZJbw</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>3.763548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153190</th>\n",
       "      <td>429494</td>\n",
       "      <td>Do provide evidence of all these claims. It appears that you are just repeating the talking heads rhetoric and that you do not have the capability of thinking on your own, especially critically</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024355</td>\n",
       "      <td>3.715011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189740</th>\n",
       "      <td>473165</td>\n",
       "      <td>KISS\\n\\nKEEP   IT   SIMPLE   STUPID\\n\\nTalk now about a SuperGrid future, but work on offering our quickest quality export product to Japan now, which is LPG-propane and AK-GTL'\\n\\nSTUB2HUB &gt; Deadhorse to Fairbanks, ASAP with flexpipe only.\\n$500million CAPEX-risk.\\n\\nSiluria turns natural gas into gasoline for $1 per gallon\\nSiluria partners with oil industry giants to make fuels cheaply\\n\\nDo Not focus on selling LNG first in 2016, keep it on the backburner.\\nFocus on micro-GTL  and  micro-GTG  plants located in Fairbanks.\\nTrying to save TAPS in it's current configuration is packing sand down a rathole.\\n\\nhttp://siluria.com/</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>3.582187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66648</th>\n",
       "      <td>323759</td>\n",
       "      <td>Why do you fucking nazi's care about his Grammer so much</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.998348</td>\n",
       "      <td>3.478251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196853</th>\n",
       "      <td>5578644</td>\n",
       "      <td>Slow to learn huh?</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.013079</td>\n",
       "      <td>3.472005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380042</th>\n",
       "      <td>5804474</td>\n",
       "      <td>you can't fix stupid\\nyou can't hide stupid\\nbut if Ted Cruz' father is out there, he can do the next best thing</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.016117</td>\n",
       "      <td>3.442594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070112</th>\n",
       "      <td>5424220</td>\n",
       "      <td>Will nobody rid me of this meedlesome FBI Director?</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>3.414973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562177</th>\n",
       "      <td>6033133</td>\n",
       "      <td>About the same percentage as Trumplodites in America?</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>3.356426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531573</th>\n",
       "      <td>893652</td>\n",
       "      <td>Yah just delete them.</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.015416</td>\n",
       "      <td>3.340996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519082</th>\n",
       "      <td>5980136</td>\n",
       "      <td>Not political? My aching posterior. This article pure watermelon. Green on the outside, red on the inside.</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>3.339497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761421</th>\n",
       "      <td>6280487</td>\n",
       "      <td>Keep spending your non-existent money, sheepies. Get that debt-to-income ratio at a new record next quarter!</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>3.313484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319769</th>\n",
       "      <td>634100</td>\n",
       "      <td>Keep your nose out of the MRI business as if Saskatchewan has not had a two tier system over the years with workmen compensation, or sports teams being the first in line.  Now that there are private clinics \\nshortening the waiting list I would suggest it benefits everybody.  Previously people went out of the province being too ill to wait a year to get diagnosed.  It is our right to get looked after in the fastest way possible.  I am sure our Health Minister and her family would find it unacceptable to wait months for an MRI.</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.016295</td>\n",
       "      <td>3.296816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673702</th>\n",
       "      <td>6173720</td>\n",
       "      <td>Is there at way they could \"fire\" each other ?!\\nSimultaneously  .-)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037293</td>\n",
       "      <td>3.288941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183576</th>\n",
       "      <td>465773</td>\n",
       "      <td>\"intending to ‚Äúget high one last time....\" Cough**bullshet**</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>3.270814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404003</th>\n",
       "      <td>5834429</td>\n",
       "      <td>My personal opinion is not so much character, but coaching.  Hamilton is desperate and desperate times calls for desperate measures.  Not downplaying his off the field antics, but they hired him for his field presence and influence over football players (beside volleyball players).</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041246</td>\n",
       "      <td>3.188204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186602</th>\n",
       "      <td>469329</td>\n",
       "      <td>why is he still alive?</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>3.175480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027816</th>\n",
       "      <td>5373866</td>\n",
       "      <td>WHO THE FUCK CARES!</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.996444</td>\n",
       "      <td>3.125973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407204</th>\n",
       "      <td>740949</td>\n",
       "      <td>Stupidiity becomes you nicely.</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>3.124892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660125</th>\n",
       "      <td>6156988</td>\n",
       "      <td>Butter me up first and insert gently please!!</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>3.077840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489027</th>\n",
       "      <td>5942447</td>\n",
       "      <td>Stop talking sense, people want to get there grove on regardless of the consequences. \\nOne word LOOOOOSERS !!!!!!</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.023045</td>\n",
       "      <td>3.020916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588060</th>\n",
       "      <td>6065292</td>\n",
       "      <td>Pushing your 86 year old mother in law down the stairs is another option.</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>3.015573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363935</th>\n",
       "      <td>5783472</td>\n",
       "      <td>You would hope so but many of his supporters are 1diots and they still support him.</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.013740</td>\n",
       "      <td>3.005350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    ...      logloss\n",
       "190446   474051     ...     4.429653\n",
       "860284   5172977    ...     4.290715\n",
       "9110     253508     ...     4.207807\n",
       "1664253  6162037    ...     4.135787\n",
       "1492735  5947182    ...     4.077752\n",
       "76512    336206     ...     4.028418\n",
       "1240658  5630606    ...     3.865027\n",
       "960637   5293418    ...     3.781680\n",
       "973847   5309299    ...     3.763548\n",
       "153190   429494     ...     3.715011\n",
       "189740   473165     ...     3.582187\n",
       "66648    323759     ...     3.478251\n",
       "1196853  5578644    ...     3.472005\n",
       "1380042  5804474    ...     3.442594\n",
       "1070112  5424220    ...     3.414973\n",
       "1562177  6033133    ...     3.356426\n",
       "531573   893652     ...     3.340996\n",
       "1519082  5980136    ...     3.339497\n",
       "1761421  6280487    ...     3.313484\n",
       "319769   634100     ...     3.296816\n",
       "1673702  6173720    ...     3.288941\n",
       "183576   465773     ...     3.270814\n",
       "1404003  5834429    ...     3.188204\n",
       "186602   469329     ...     3.175480\n",
       "1027816  5373866    ...     3.125973\n",
       "407204   740949     ...     3.124892\n",
       "1660125  6156988    ...     3.077840\n",
       "1489027  5942447    ...     3.020916\n",
       "1588060  6065292    ...     3.015573\n",
       "1363935  5783472    ...     3.005350\n",
       "\n",
       "[30 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "# for fold, (train_index, valid_index) in enumerate(cv.split(range(len(train)), np.where(train[target]>0.5, 1, 0))):\n",
    "#     if fold==n_fold:\n",
    "valid = train.loc[valid_index]\n",
    "valid['pred'] = np.load(f'pred_valid_fold_{fold}.npy')[:, 0]\n",
    "valid['logloss'] = log_loss(valid[target].values, valid['pred'].values)\n",
    "\n",
    "valid.sort_values('logloss', ascending=False).loc[:, ['id', 'comment_text', 'target', 'pred', 'logloss']].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(y_pred):\n",
    "    sub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
    "    sub['prediction'] = y_pred[:, 0]\n",
    "    sub.to_csv('submission.csv', index=False)\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = prepare_data_loader(X_test, test_lengths, shuffle=False)\n",
    "\n",
    "pred_tests = []\n",
    "weights = sorted(glob.glob('weight_best_fold_*.pth'))\n",
    "for fold, path in enumerate(weights):\n",
    "    model = EmbLSTM(embedding_matrix, max_features).cuda()\n",
    "    state = torch.load(path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    \n",
    "    pred_test = predict_model(model, test_loader)\n",
    "    np.save(f'pred_test_fold_{fold}.npy', pred_test)\n",
    "    pred_tests.append(pred_test)\n",
    "    \n",
    "    del model; gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "pred_tests = np.mean(pred_tests, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.047286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.005536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.073810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.034756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.701859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000  0.047286  \n",
       "1  7000001  0.005536  \n",
       "2  7000002  0.073810  \n",
       "3  7000003  0.034756  \n",
       "4  7000004  0.701859  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = submission(pred_tests)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAELCAYAAAAhuwopAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHJhJREFUeJzt3XtwVOX9x/FPsuGixTUGJdmwWiqlNJWpZUxBK4gmaAjdJEatSRMwmlrG2gtW5SejkkRRx3AZRg2U1jo0VloqoxKJlLSCBaEVoZUCjYrloonZDZiQWeSWsHl+fzjsGNBmnySb3ZD3a2Znsud7zj7f52TmfHL27J7EGGOMAACwEBvpBgAAfQ/hAQCwRngAAKwRHgAAa4QHAMAa4QEAsEZ4ACGaPn26Vq5c+YW1hoYGjR07VoFAoJe7AiKD8ECflpaWpr///e/dfp2XX35ZP/zhD7u8fXJyst555x05HI6wjgNEC8IDOEsYY9Te3h7pNtBPEB7os2bNmqWGhgbdddddGjt2rJ599llJ0vbt25Wfn6/U1FRlZ2dry5YtwW1efvllpaena+zYsUpLS9Orr76qPXv2qLS0VNu3b9fYsWOVmpr6pWN+/PHHys/P19ixY1VcXKzm5mZJUn19vUaPHq2TJ09aj3P48GH93//9n6688kpdd911WrJkSTAEAoGAnnzySY0fP15paWl64YUXOowzffp0LVq0SPn5+br88stVV1enl156SZmZmRo7dqzS09O1YsWKYP9btmzRNddco2effVZXXXWVJkyYoNdff10bNmxQRkaGxo0bp6VLl/bgbwlnLQP0Ydddd53ZvHlz8LnP5zPjxo0zf/vb30wgEDCbNm0y48aNM01NTebIkSNm7NixZs+ePcYYYxobG83u3buNMca89NJLJj8//3+ONW3aNJOenm727t1rjh07ZqZNm2bmz59vjDGmrq7OfOMb3zBtbW3W48yaNcvcdddd5vDhw6aurs7ccMMN5sUXXzTGGPOHP/zBZGZmGq/Xa1paWkxRUVFwnFM9TZo0yezevdu0tbWZ1tZW88Ybb5gPP/zQtLe3my1btphvf/vbZteuXcYYY9566y2TkpJinnnmGdPa2mr+9Kc/mfHjx5t7773XHD582OzevduMGTPGfPTRR936veDsx5kHzipVVVW65pprNGnSJMXGxurqq6/WmDFjtGHDBklSbGysPvjgAx0/flzDhg3TqFGjrF7/pptu0te+9jUNHjxYU6ZM0bvvvvuF64U6TiAQ0Jo1a3TfffdpyJAhcrvduuOOO/Tqq69Kkv785z/rtttuU1JSks4//3zNmDHjjNfIzc3VqFGjFBcXpwEDBujaa6/VJZdcopiYGI0bN05XX321tm3bFlw/Li5OP/nJTzRgwABNnTpVhw4d0m233aYhQ4Zo1KhRGjVqlN5//32r/YL+h/DAWaWhoUFr165Vampq8PHPf/5TBw8e1LnnnqtFixZpxYoVmjBhgmbMmKE9e/ZYvf5FF10U/Pmcc87R0aNHz1jHZpxDhw6pra1NycnJwWXJyclqbGyUJB04cEAulytYS0pKOuM1Pl+XpA0bNujWW2/VuHHjlJqaqo0bN+rQoUPBenx8fPDC/uDBgyVJQ4cODdYHDRqkI0eOfPlOAER44CzjcrmUk5Ojbdu2BR/bt28P/sU+ceJELVu2TJs2bdKll16qOXPmSJJiYmJ6tI9Qx7ngggs0YMAANTQ0BJd5vV4lJiZK+iysfD5fsPb5n0/5/Gu2trbqF7/4hYqLi7V582Zt27ZN11xzjQw3z0YPIzzQp1144YWqq6sLPs/OztYbb7yhN998U4FAQCdOnNCWLVvk8/n0ySefaN26dTp69KgGDhyoc889N/gX+NChQ9XY2KjW1tZu92QzjsPh0JQpU7Ro0SJ9+umn+vjjj7Vs2TJlZ2dLkjIzM/X888+rsbFRfr8/+KGAL9Pa2qrW1lYlJCQoLi5OGzZs0ObNm7s9J+B0hAf6tBkzZuhXv/qVUlNT9dxzz8nlcmnJkiX69a9/rauuukqTJk3Sc889p/b2drW3t2vZsmWaOHGixo0bp61bt6q0tFSSdOWVV+rrX/+6JkyYoPHjx3erJ9tx5syZo3POOUeTJ09WQUGBPB6Pbr75ZknSrbfeqquvvlrZ2dm68cYbNWnSJMXFxX3p90mGDBmihx9+WPfcc4+++93vqrq6Wmlpad2aD/BFYgzns0CfsWHDBpWVlemNN96IdCvo5zjzAKLY8ePHtWHDBp08eVKNjY1avHixJk+eHOm2AM48gGh27NgxTZs2TXv37tXgwYN17bXX6qGHHtKQIUMi3Rr6OcIDAGCNt60AANbiIt1ATzp+/Lh27dqliy66qNO7mwIAPhMIBHTw4EGNGTMm+MXRzpxV4bFr1y4VFhZGug0A6JOWL1/+P28M+nlnVXicunXE8uXLv/A2DgCAM/l8PhUWFna4/U5nzqrwOPVWVVJSktxud4S7AYC+xebtfi6YAwCsER4AAGuEBwDAGuEBALBGeAAArBEeAABrhAcAwBrh8TmtbYF+NS4AdNVZ9SXB7ho4wKGs+6p6fdzVC3N6fUwA6I5Ow6O+vl4//elPg88PHz6sTz/9VG+//bb27dun2bNnq6WlRfHx8SovL9eIESMkKSw1AEB06PRtK7fbraqqquAjPT1dHo9HklRaWqqCggLV1NSooKBAJSUlwe3CUQMARAerax6tra1avXq1br75ZjU1Nam2tjYYJB6PR7W1tWpubg5LDQAQPayueaxfv16JiYm67LLLtGvXLiUmJgZvpOVwODRs2DB5vV4ZY3q8lpCQ0KEXv98vv9/fYZnP5+vaXgAAWLEKj5deekk333xzuHqxUllZqYqKiki3AQD9Usjh0djYqK1bt2revHmSJJfLpcbGRgUCATkcDgUCAR04cEAul0vGmB6vna6oqEi5ubkdlp26Jz0AILxCvubxyiuvaNKkSbrgggskSUOHDlVKSoqqq6slSdXV1UpJSVFCQkJYaqdzOp1yu90dHvwDKADoHSGfebzyyit66KGHOiwrKyvT7NmztWTJEjmdTpWXl4e1BgCIDiGHR01NzRnLRo4cqZUrV37h+uGoAQCiA7cnAQBYIzwAANYIDwCANcIDAGCN8AAAWCM8AADWCA8AgDXCAwBgjfAAAFgjPAAA1ggPAIA1wgMAYI3wAABYIzwAANYIDwCANcIDAGCN8AAAWCM8AADWCA8AgDXCAwBgLaTwOHHihEpLS3XDDTcoKytLc+bMkSTt27dPeXl5ysjIUF5envbv3x/cJhw1AEB0CCk85s+fr0GDBqmmpkarV6/WzJkzJUmlpaUqKChQTU2NCgoKVFJSEtwmHDUAQHToNDyOHDmiVatWaebMmYqJiZEkXXjhhWpqalJtba08Ho8kyePxqLa2Vs3NzWGpAQCiR1xnK9TV1Sk+Pl4VFRXasmWLvvKVr2jmzJkaPHiwEhMT5XA4JEkOh0PDhg2T1+uVMabHawkJCR368vv98vv9HZb5fL7u7xEAQKc6DY+TJ0+qrq5O3/rWt/TAAw/o3//+t+666y499dRTvdHfl6qsrFRFRUVEewCA/qrT8EhOTlZcXFzwraTLL79cF1xwgQYPHqzGxkYFAgE5HA4FAgEdOHBALpdLxpger52uqKhIubm5HZb5fD4VFhb20K4BAHyZTq95JCQkaPz48dq8ebOkzz4N1dTUpBEjRiglJUXV1dWSpOrqaqWkpCghIUFDhw7t8drpnE6n3G53h0dSUlLP7BUAwP8UY4wxna1UV1enBx98UC0tLYqLi9M999yjSZMmac+ePZo9e7b8fr+cTqfKy8t16aWXSlJYap2pr69Xenq61q1bJ7fb3aUdknVfVZe2647VC3N6fUwAOKUrx86QwqOvIDwAwF5Xjp18wxwAYI3wAABYIzwAANYIDwCANcIDAGCN8AAAWCM8AADWCA8AgDXCAwBgjfAAAFgjPAAA1ggPAIA1wgMAYI3wAABYIzwAANYIDwCANcIDAGCN8AAAWCM8AADWCA8AgLWQwiMtLU1TpkxRTk6OcnJy9Oabb0qStm/fruzsbGVkZKi4uFhNTU3BbcJRAwBEh5DPPJ5++mlVVVWpqqpKEydOlDFGs2bNUklJiWpqapSamqoFCxZIUlhqAIDo0eW3rXbu3KlBgwYpNTVVkpSfn6+1a9eGrXY6v9+v+vr6Dg+fz9fV6QAALMSFuuL9998vY4yuuOIK3XvvvfJ6vUpOTg7WExIS1N7erpaWlrDU4uPjO/RTWVmpioqKLk0aANA9IYXH8uXL5XK51Nraqscff1yPPvqorr/++nD39j8VFRUpNze3wzKfz6fCwsIIdQQA/UdIb1u5XC5J0sCBA1VQUKB//etfcrlcamhoCK7T3NysmJgYxcfHh6V2OqfTKbfb3eGRlJRkvwcAANY6DY+jR4/q8OHDkj67oL1mzRqlpKRozJgxOn78uLZt2yZJWrFihTIzMyUpLDUAQPTo9G2rpqYm/fznP1cgEFB7e7tGjhyp0tJSxcbGat68eSotLdWJEyc0fPhwzZ8/X5LCUgMARI8YY4yJdBM9pb6+Xunp6Vq3bp3cbneXXiPrvqoe7qpzqxfm9PqYAHBKV46dfMMcAGCN8AAAWCM8AADWCA8AgDXCAwBgjfAAAFgjPAAA1ggPAIA1wgMAYI3wAABYIzwAANYIDwCANcIDAGCN8AAAWCM8AADWCA8AgDXCAwBgjfAAAFgjPAAA1qzCo6KiQqNHj9bu3bslSdu3b1d2drYyMjJUXFyspqam4LrhqAEAokPI4fGf//xH27dvV3JysiTJGKNZs2appKRENTU1Sk1N1YIFC8JWAwBEj5DCo7W1VY8++qhKS0sVExMjSdq5c6cGDRqk1NRUSVJ+fr7Wrl0bthoAIHrEhbLSU089pezsbF188cXBZV6vN3gWIkkJCQlqb29XS0tLWGrx8fEdevL7/fL7/R2W+Xy+EKcNAOiOTsPjnXfe0c6dO3X//ff3Rj8hq6ysVEVFRaTbAIB+qdPw2Lp1q/bu3av09HRJn/11/6Mf/UjTp09XQ0NDcL3m5mbFxMQoPj5eLperx2unKyoqUm5ubodlPp9PhYWFFtMHAHRFp9c8ZsyYoU2bNmn9+vVav369kpKS9Nxzz+nOO+/U8ePHtW3bNknSihUrlJmZKUkaM2ZMj9dO53Q65Xa7OzySkpK6sy8AACEK6ZrHF4mNjdW8efNUWlqqEydOaPjw4Zo/f37YagCA6BFjjDGRbqKn1NfXKz09XevWrZPb7e7Sa2TdV9XDXXVu9cKcXh8TAE7pyrGTb5gDAKwRHgAAa4QHAMAa4QEAsEZ4AACsER4AAGuEBwDAGuEBALBGeAAArBEeAABrhAcAwBrhAQCwRngAAKwRHgAAa4QHAMAa4QEAsEZ4AACsER4AAGuEBwDAGuEBALAWUnjcfffdys7O1o033qiCggK9++67kqR9+/YpLy9PGRkZysvL0/79+4PbhKMGAIgOIYVHeXm5Xn31Va1atUrFxcV68MEHJUmlpaUqKChQTU2NCgoKVFJSEtwmHDUAQHQIKTzOO++84M+ffvqpYmJi1NTUpNraWnk8HkmSx+NRbW2tmpubw1IDAESPuFBXfOihh7R582YZY/Tb3/5WXq9XiYmJcjgckiSHw6Fhw4bJ6/XKGNPjtYSEhA79+P1++f3+Dst8Pl/X9wQAIGQhh8fjjz8uSVq1apXmzZunmTNnhq2pUFRWVqqioiKiPQBAfxVyeJxy4403qqSkRElJSWpsbFQgEJDD4VAgENCBAwfkcrlkjOnx2umKioqUm5vbYZnP51NhYWHX9wYAICSdXvM4cuSIvF5v8Pn69et1/vnna+jQoUpJSVF1dbUkqbq6WikpKUpISAhL7XROp1Nut7vDIykpqft7BADQqU7PPI4dO6aZM2fq2LFjio2N1fnnn6+lS5cqJiZGZWVlmj17tpYsWSKn06ny8vLgduGoAQCiQ4wxxkS6iZ5SX1+v9PR0rVu3Tm63u0uvkXVfVQ931bnVC3N6fUwAOKUrx06+YQ4AsEZ4AACsER4AAGuEBwDAGuEBALBGeAAArBEeAABrhAcAwBrhAQCwRngAAKwRHgAAa4QHAMAa4QEAsEZ4AACsER4AAGuEBwDAGuEBALBGeAAArBEeAABrhAcAwFqn4XHo0CH9+Mc/VkZGhrKysvSzn/1Mzc3NkqTt27crOztbGRkZKi4uVlNTU3C7cNQAANGh0/CIiYnRnXfeqZqaGq1evVoXX3yxFixYIGOMZs2apZKSEtXU1Cg1NVULFiyQpLDUAADRo9PwiI+P1/jx44PPv/Od76ihoUE7d+7UoEGDlJqaKknKz8/X2rVrJSksNQBA9IizWbm9vV1//OMflZaWJq/Xq+Tk5GAtISFB7e3tamlpCUstPj6+Qy9+v19+v7/DMp/PZzMdAEAXWYXH3Llzde6552ratGn661//Gq6eQlJZWamKioqI9gAA/VXI4VFeXq4PP/xQS5cuVWxsrFwulxoaGoL15uZmxcTEKD4+Piy10xUVFSk3N7fDMp/Pp8LCwlCnBADoopA+qrto0SLt2rVLixcv1sCBAyVJY8aM0fHjx7Vt2zZJ0ooVK5SZmRm22umcTqfcbneHR1JSUpd2AgDATqdnHh988IGWLl2qESNGKD8/X5Lkdru1ePFizZs3T6WlpTpx4oSGDx+u+fPnS5JiY2N7vAYAiB4xxhgT6SZ6Sn19vdLT07Vu3Tq53e4uvUbWfVU93FXnVi/M6fUxAeCUrhw7+YY5AMAa4QEAsEZ4AACsER4AAGuEBwDAGuEBALBGeAAArBEeAABrhAcAwBrhAQCwRngAAKwRHgAAa4QHAMAa4QEAsEZ4AACsER4AAGuEBwDAGuEBALBGeAAArBEeAABrnYZHeXm50tLSNHr0aO3evTu4fN++fcrLy1NGRoby8vK0f//+sNYAANGj0/BIT0/X8uXLNXz48A7LS0tLVVBQoJqaGhUUFKikpCSsNQBA9Og0PFJTU+VyuTosa2pqUm1trTwejyTJ4/GotrZWzc3NYakBAKJLXFc28nq9SkxMlMPhkCQ5HA4NGzZMXq9XxpgeryUkJJzRg9/vl9/v77DM5/N1ZToAAEtdCo9oUFlZqYqKiki3AQD9UpfCw+VyqbGxUYFAQA6HQ4FAQAcOHJDL5ZIxpsdrX6SoqEi5ubkdlvl8PhUWFnZlShHV2hbQwAGOfjc2gL6rS+ExdOhQpaSkqLq6Wjk5OaqurlZKSkrw7aVw1E7ndDrldDq70n7UGTjAoaz7qiIy9uqFOREZF0Df1ml4PPbYY/rLX/6iTz75RHfccYfi4+P12muvqaysTLNnz9aSJUvkdDpVXl4e3CYcNQBA9Og0PB5++GE9/PDDZywfOXKkVq5c+YXbhKMGAIgefMMcAGCN8AAAWCM8AADWCA8AgDXCAwBgjfAAAFgjPAAA1ggPAIA1wgMAYI3wAABYIzz6uda2QL8aF0DP6LP/zwM9I1J39OVuvkDfxpkHAMAa4QEAsEZ4ICIiec2D6y1A93HNAxHBf08E+jbOPNDv8AkzoPs480C/wyfMgO7jzAPoJVznwdmEMw+gl0TyOs9LT3oiMm5rW0ADBzgiMjbCKyrDY9++fZo9e7ZaWloUHx+v8vJyjRgxItJtAX1WpIKL0Dp7RWV4lJaWqqCgQDk5OaqqqlJJSYmef/75SLcFwFJ/Cy2p/wRX1IVHU1OTamtrtWzZMkmSx+PR3Llz1dzcrISEhOB6fr9ffr+/w7Yff/yxJMnn83V5/LajzV3etqvq6+sjMm4kx2bO/WPsSI17oNGrOx//a6+PK0m/fej6iIzbHaeOmYFA6NfGYowxJlwNdcWuXbv0wAMP6LXXXgsumzp1qubPn6/LLrssuOyZZ55RRUVFJFoEgLPS8uXLlZqaGtK6UXfmEaqioiLl5uZ2WNba2qq6ujqNGDFCDofdaaPP51NhYaGWL1+upKSknmw1ajFn5ny2Ys52cw4EAjp48KDGjBkT8jZRFx4ul0uNjY0KBAJyOBwKBAI6cOCAXC5Xh/WcTqecTucZ21966aXdGj8pKUlut7tbr9HXMOf+gTn3D12d81e/+lWr9aPuex5Dhw5VSkqKqqurJUnV1dVKSUnpcL0DABBZUXfmIUllZWWaPXu2lixZIqfTqfLy8ki3BAD4nKgMj5EjR2rlypWRbgMA8CUcZWVlZZFuIloMGjRI48eP16BBgyLdSq9hzv0Dc+4fenPOUfdRXQBA9Iu6C+YAgOhHeAAArPW78Ni3b5/y8vKUkZGhvLw87d+//4x1AoGAHnnkEU2ePFnXX399n794H8qcFy9erO9///vKzs7WTTfdpDfffLP3G+1Bocz5lL179+ryyy/v85/qC3XOa9asUVZWljwej7KysvTJJ5/0bqM9KJQ5NzU1acaMGcrKytKUKVNUVlamkydP9n6zPaC8vFxpaWkaPXq0du/e/YXr9Nrxy/Qz06dPN6tWrTLGGLNq1Sozffr0M9Z55ZVXTHFxsQkEAqapqclMnDjR1NXV9XarPSaUOW/cuNEcPXrUGGPMu+++a6644gpz7NixXu2zJ4UyZ2OMOXnypJk2bZq59957zZNPPtmbLfa4UOa8Y8cOk5mZaQ4cOGCMMcbv95vjx4/3ap89KZQ5P/bYY8HfbWtrq7nlllvMa6+91qt99pStW7eahoYGc91115n333//C9fpreNXvzrzOHXTRY/nsztuejwe1dbWqrm5443b1qxZox/84AeKjY1VQkKCJk+erLVr10ai5W4Ldc4TJ07UOeecI0kaPXq0jDFqaWnp9X57QqhzlqTf/OY3uvbaa/v8Lf9DnfPvfvc7FRcX66KLLpIknXfeeX3200ihzjkmJkZHjhxRe3u7Wltb1dbWpsTExEi03G2pqaln3G3jdL11/OpX4eH1epWYmBi875XD4dCwYcPk9XrPWC85OTn43OVydetOvZEU6pw/b9WqVbrkkkv67D2BQp3ze++9p02bNun222+PQJc9K9Q579mzR3V1dSosLFRubq6WLFki00c/cBnqnO+++27t27dPEyZMCD6uuOKKSLTcK3rr+NWvwgOde/vtt/XUU09p4cKFkW4lrNra2jRnzhw98sgj1jfR7MsCgYDef/99LVu2TL///e+1ceNGVVVF5r8b9pa1a9dq9OjR2rRpkzZu3Kht27b12XcSokm/Co/P33RR0pfedNHlcqmhoSH43Ov19tm/wkOdsyS98847mjVrlhYvXtztG0xGUihzPnjwoD766CPNmDFDaWlpqqys1Isvvqg5c+ZEqu1uCfX3nJycrClTpmjgwIEaMmSI0tPTtWPHjki03G2hzvmFF15Qdna2YmNjdd555yktLU1btmyJRMu9oreOX/0qPEK96eKUKVO0cuVKtbe3q7m5Wa+//royMjIi0XK3hTrnHTt26Je//KWefvrpDv83pS8KZc7JycnasmWL1q9fr/Xr16uoqEi33nqr5s6dG6m2uyXU37PH49GmTZtkjFFbW5veeustffOb34xEy90W6pzdbrc2btwo6bN/2/CPf/xDo0aN6vV+e0uvHb96/BJ8lPvvf/9rbrnlFnPDDTeYW265xezZs8cYY8ydd95pduzYYYz57BM4JSUlJj093aSnp5sVK1ZEsuVuC2XON910kxk/frzJzs4OPt57771Itt0tocz5855++uk+/2mrUOYcCATME088YaZMmWKmTp1qnnjiCRMIBCLZdreEMucPP/zQ3H777cbj8ZjMzExTVlZm2traItl2l82dO9dMnDjRpKSkmO9973tm6tSpxpjIHL+4PQkAwFq/etsKANAzCA8AgDXCAwBgjfAAAFgjPAAA1ggPAIA1wgMAYI3wAABY+38j/IqYEaJdxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.hist(pred_tests[:, 0])\n",
    "plt.title('test histogram')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
