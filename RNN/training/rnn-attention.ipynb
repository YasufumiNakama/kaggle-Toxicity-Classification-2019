{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_rate = 0.4\n",
    "# warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from keras.preprocessing import text, sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ildoonet/pytorch-gradual-warmup-lr/blob/master/warmup_scheduler/scheduler.py\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class GradualWarmupScheduler(_LRScheduler):\n",
    "    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n",
    "    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        multiplier: target learning rate = base lr * multiplier\n",
    "        total_epoch: target learning rate is reached at total_epoch, gradually\n",
    "        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        if self.multiplier <= 1.:\n",
    "            raise ValueError('multiplier should be greater than 1.')\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "\n",
    "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
    "        if self.last_epoch <= self.total_epoch:\n",
    "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            if epoch is None:\n",
    "                self.after_scheduler.step(metrics, None)\n",
    "            else:\n",
    "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
    "\n",
    "    def step(self, epoch=None, metrics=None):\n",
    "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
    "            if self.finished and self.after_scheduler:\n",
    "                if epoch is None:\n",
    "                    self.after_scheduler.step(None)\n",
    "                else:\n",
    "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
    "            else:\n",
    "                return super(GradualWarmupScheduler, self).step(epoch)\n",
    "        else:\n",
    "            self.step_ReduceLROnPlateau(metrics, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    # handler1\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(asctime)s %(levelname)8s %(message)s\"))\n",
    "    # handler2\n",
    "    handler2 = FileHandler(filename=datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")+\".log\")\n",
    "    handler2.setFormatter(Formatter(\"%(asctime)s %(levelname)8s %(message)s\"))\n",
    "    # addHandler\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log_loss(y_true, y_pred, epsilon=1e-12):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "    return -(np.log(y_pred) * y_true + np.log(1 - y_pred) * (1 - y_true))\n",
    "\n",
    "class JigsawEvaluator:\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = (y_true >= 0.5).astype(int)\n",
    "        self.y_i = (y_identity >= 0.5).astype(int)\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score, bias_metrics[0], bias_metrics[1], bias_metrics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger\n",
    "logger = get_logger()\n",
    "\n",
    "# parameters\n",
    "n_workers = 4\n",
    "n_splits = 5\n",
    "seed = 777\n",
    "seed_everything(seed)\n",
    "\n",
    "maxlen = 300\n",
    "max_features = 410047\n",
    "\n",
    "batch_size = 512\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "n_fold = 1\n",
    "drop_rate = 0.4 # default=0.3\n",
    "\n",
    "# path\n",
    "CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\n",
    "GLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n",
    "# GOOGLE_EMBEDDING_PATH = '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "# WIKI_EMBEDDING_PATH = '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "\n",
    "# constants\n",
    "target = 'target'\n",
    "aux_target = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (1804874, 45)\n",
      "test shape: (97320, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            ...             toxicity_annotator_count\n",
       "0  59848            ...                                    4\n",
       "1  59849            ...                                    4\n",
       "2  59852            ...                                    4\n",
       "\n",
       "[3 rows x 45 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "print(f'train shape: {train.shape}')\n",
    "print(f'test shape: {test.shape}')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>Jeff Sessions is another one of Trump's Orwell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>I actually inspected the infrastructure on Gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>No it won't . That's just wishful thinking on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text\n",
       "0  7000000  Jeff Sessions is another one of Trump's Orwell...\n",
       "1  7000001  I actually inspected the infrastructure on Gra...\n",
       "2  7000002  No it won't . That's just wishful thinking on ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "y shape: (1804874, 8)\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "weights = np.ones((len(train),)) / 4\n",
    "\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (((train[target].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (1-(train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int)) ) > 1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (((train[target].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "#loss_weight = 1.0 / weights.mean()\n",
    "loss_weight = 0.5\n",
    "print(loss_weight)\n",
    "\n",
    "y_train = np.vstack([train[target], weights]).T\n",
    "# y_train = np.vstack([np.where(train[target]>=0.5, train[target], 0), weights]).T\n",
    "y_aux_train = train[[target]+aux_target]\n",
    "y_train = np.hstack([y_train, y_aux_train])\n",
    "print(f'y shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('../input/toxicpreprocesseddata/X_train.pkl')\n",
    "X_test = pd.read_pickle('../input/toxicpreprocesseddata/X_test.pkl')\n",
    "embedding_matrix = pd.read_pickle('../input/toxicpreprocesseddata/embedding_matrix.pkl')\n",
    "tokenizer = pd.read_pickle('../input/toxicpreprocesseddata/tokenizer.pkl')\n",
    "train_lengths = pd.read_pickle('../input/toxicpreprocesseddata/train_lengths.pkl')\n",
    "test_lengths = pd.read_pickle('../input/toxicpreprocesseddata/test_lengths.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch\n",
    "    \n",
    "def prepare_data_loader(X, lengths, y=None, shuffle=False):\n",
    "    if y is None:\n",
    "        dataset = TensorDataset(torch.from_numpy(X), \n",
    "                                torch.from_numpy(lengths))\n",
    "        collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n",
    "                                          sequence_index=0, \n",
    "                                          length_index=1)\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X), \n",
    "                                torch.from_numpy(lengths), \n",
    "                                torch.tensor(y, dtype=torch.float32))\n",
    "        collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n",
    "                                          sequence_index=0, \n",
    "                                          length_index=1, \n",
    "                                          label_index=2)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(data, targets):\n",
    "    ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:, 1:2])(data[:, :1], targets[:, :1])\n",
    "    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:, 1:], targets[:, 2:])\n",
    "    return (bce_loss_1 * loss_weight) + bce_loss_2\n",
    "\n",
    "#def custom_loss(data, targets):\n",
    "#    ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "#    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:, 1:2])(data[:, :1], targets[:, :1])\n",
    "#    return bce_loss_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filepath, model, optimizer, epoch):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "def plot_losses(train_losses, valid_losses, fold=0):\n",
    "    plt.clf()\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(valid_losses, label='valid')\n",
    "    plt.legend()\n",
    "    plt.title(f'loss history of fold {fold}')\n",
    "    plt.savefig(f'loss_history_of_fold_{fold}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_histogram(y_true, y_pred, n_bins=50, fold=0):\n",
    "    bins = np.linspace(0, 1, n_bins)\n",
    "    plt.clf()\n",
    "    plt.hist(y_pred[:, 0], bins=bins, label='pred')\n",
    "    plt.hist(y_true[:, 0], bins=bins, label='true')\n",
    "    plt.legend()\n",
    "    plt.title(f'validation histogram of fold {fold}')\n",
    "    plt.savefig(f'validation_histogram_of_fold_{fold}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_aucs(aucs, auc_type, fold=0):\n",
    "    total = sum(np.power(aucs, -5))\n",
    "    score = np.power(total / len(aucs), 1 / -5)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    ax = sns.barplot(identity_columns, aucs)\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.4f}', (p.get_x()+p.get_width()/2, int(p.get_height()*0.95)),\n",
    "                    ha='center', va='center', fontsize=20, color='blue', xytext=(0, 20), \n",
    "                    textcoords='offset points')\n",
    "    plt.xticks(rotation=10)\n",
    "    plt.title(f'{auc_type} {score} barplot of fold {fold}')\n",
    "    plt.savefig(f'{auc_type}_{score}_barplot_of_fold_{fold}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, X):\n",
    "    logits = model(X)\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    return logits, probabilities\n",
    "        \n",
    "def evaluate_single_epoch(model, dataloader, criterion, epoch, evaluator, return_pred=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_preds = []\n",
    "        loss_list = []\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            # for SequenceBucketCollator\n",
    "            #X = X[0].cuda().long() # X[0]: text sequences, X[1]: lengths\n",
    "            # new\n",
    "            X = X.cuda().long()\n",
    "            y = y.cuda().float()\n",
    "            logits, probabilities = inference(model, X)\n",
    "            \n",
    "            loss = criterion(logits, y)\n",
    "            loss_list.append(loss.item())\n",
    "            valid_preds.append(probabilities.cpu().numpy())\n",
    "        valid_preds = np.concatenate(valid_preds)\n",
    "\n",
    "        log_dict = {}\n",
    "        score, subgroup_auc, bpsn_auc, bnsp_auc = evaluator.get_final_metric(valid_preds[:, 0])\n",
    "        log_dict['score'] = score\n",
    "        log_dict['subgroup_auc'] = subgroup_auc\n",
    "        log_dict['bpsn_auc'] = bpsn_auc\n",
    "        log_dict['bnsp_auc'] = bnsp_auc\n",
    "        log_dict['loss'] = np.mean(loss_list)\n",
    "        \n",
    "        if return_pred:\n",
    "            log_dict['pred'] = valid_preds\n",
    "            \n",
    "    return log_dict\n",
    "\n",
    "def train_single_epoch(model, dataloader, criterion, optimizer, epoch, parent_bar, scheduler=None):\n",
    "    model.train()\n",
    "    log_dict = {}\n",
    "    log_dict['loss'] = 0\n",
    "    for X, y in progress_bar(dataloader, parent=parent_bar):\n",
    "        # for SequenceBucketCollator\n",
    "        #X = X[0].cuda().long() # X[0]: text sequences, X[1]: lengths\n",
    "        # new\n",
    "        X = X.cuda().long()\n",
    "        y = y.cuda().float()\n",
    "        logits, probabilities = inference(model, X)\n",
    "        \n",
    "        loss = criterion(logits, y)\n",
    "        log_dict['loss'] += loss.item() / len(dataloader)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        log_dict['lr'] = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "        \n",
    "    return log_dict\n",
    "\n",
    "def predict_model(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for i, X in enumerate(dataloader):\n",
    "            # for SequenceBucketCollator\n",
    "            #X = X[0].cuda().long() # X[0]: text sequences, X[1]: lengths\n",
    "            # new\n",
    "            X = X.cuda().long()\n",
    "            logits, probabilities = inference(model, X)\n",
    "            test_preds.append(probabilities.cpu().numpy())\n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = train.copy()\n",
    "kfold['fold_id'] = 0\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, valid_index) in enumerate(cv.split(range(len(train)), np.where(train[target]>0.5, 1, 0))):\n",
    "    kfold.loc[valid_index, 'fold_id'] = fold\n",
    "kfold[['fold_id']].to_csv('fold01.csv', index=False)\n",
    "\n",
    "del kfold; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "# https://www.kaggle.com/artgor/text-modelling-in-pytorch\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim),\n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class EmbLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_features, num_aux_targets=6):\n",
    "        super().__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(drop_rate)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        #self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm1_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.lstm2_attention = Attention(hidden_size * 2, maxlen)\n",
    "    \n",
    "        #self.linear1 = nn.Linear(hidden_size * 8, hidden_size * 8)\n",
    "        #self.linear2 = nn.Linear(hidden_size * 8, hidden_size * 8)\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size * 8),\n",
    "            nn.Linear(hidden_size * 8, hidden_size * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size * 8),\n",
    "            nn.Linear(hidden_size * 8, hidden_size * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.linear_out = nn.Linear(hidden_size * 8, 1)\n",
    "        self.linear_aux_out = nn.Linear(hidden_size * 8, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"x\", x.size())\n",
    "        h_embedding = self.embedding(x)\n",
    "        #print(\"h_embedding\", h_embedding.size())\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        #print(\"h_embedding\", h_embedding.size())\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        #print(\"h_lstm1\", h_lstm1.size())\n",
    "        #print(\"h_lstm2\", h_lstm2.size())\n",
    "        \n",
    "        #h_lstm1_atten = self.lstm1_attention(torch.reshape(h_lstm1, (hidden_size * 4, -1)))\n",
    "        #h_lstm2_atten = self.lstm2_attention(torch.reshape(h_lstm2, (hidden_size * 4, -1)))\n",
    "        h_lstm1_atten = self.lstm1_attention(h_lstm1)\n",
    "        h_lstm2_atten = self.lstm2_attention(h_lstm2)\n",
    "        #print(\"h_lstm1_atten\", h_lstm1_atten.size())\n",
    "        #print(\"h_lstm2_atten\", h_lstm2_atten.size())\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        #print(\"avg_pool\", avg_pool.size())\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        #print(\"max_pool\", max_pool.size())\n",
    "        \n",
    "        h_conc = torch.cat((h_lstm1_atten, h_lstm2_atten, max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        #print(\"h_conc\", h_conc.size())\n",
    "        #print(\"h_conc_linear1\", h_conc_linear1.size())\n",
    "        #print(\"h_conc_linear2\", h_conc_linear2.size())\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        #print(\"hidden\", hidden.size())\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        \n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class EmbLSTMGRUCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_features, num_aux_targets=6):\n",
    "        super().__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(drop_rate)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, 80, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.GRU(80*2, 80, bidirectional=True, batch_first=True)\n",
    "        self.cnn = nn.Conv1d(80*2, 64, kernel_size=3, padding=0)\n",
    "    \n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(458),\n",
    "            nn.Linear(458, 458),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(458),\n",
    "            nn.Linear(458, 458),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.linear_out = nn.Linear(458, 1)\n",
    "        self.linear_aux_out = nn.Linear(458, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        x = self.cnn(h_lstm2.permute(0, 2, 1))\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(x, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(x, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = self.linear1(h_conc)\n",
    "        h_conc_linear2  = self.linear2(h_conc)\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.00% [1/10 11:31<1:43:44]\n",
       "    </div>\n",
       "    \n",
       "Epoch 1 - avg_train_loss: 0.1506  avg_val_loss: 0.1452  val_score: 0.920832<p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='615' class='' max='2821', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      21.80% [615/2821 02:18<08:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-23 03:43:48,758     INFO Epoch 1 - avg_train_loss: 0.1506 avg_val_loss: 0.1452  val_score: 0.920832\n"
     ]
    }
   ],
   "source": [
    "#cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for fold, (train_index, valid_index) in enumerate(cv.split(range(len(train)), np.where(train[target]>0.5, 1, 0))):\n",
    "    if fold==n_fold:\n",
    "        \n",
    "        # data split\n",
    "        #X_trn, X_val = X_train[train_index], X_train[valid_index]\n",
    "        #y_trn, y_val = y_train[train_index], y_train[valid_index]\n",
    "        #trn_lengths, val_lengths = train_lengths[train_index], train_lengths[valid_index]\n",
    "        y_val_target = train.loc[valid_index, target].values\n",
    "        y_val_identity = train.loc[valid_index, identity_columns].values\n",
    "        \n",
    "        # for SequenceBucketCollator\n",
    "        #train_loader = prepare_data_loader(X_trn, trn_lengths, y=y_trn, shuffle=True)\n",
    "        #valid_loader = prepare_data_loader(X_val, val_lengths, y=y_val, shuffle=False)\n",
    "        # new same length=maxlen\n",
    "        x_train_fold = torch.tensor(X_train[train_index], dtype=torch.long).cuda()\n",
    "        y_train_fold = torch.tensor(y_train[train_index], dtype=torch.float32).cuda()\n",
    "        x_val_fold = torch.tensor(X_train[valid_index], dtype=torch.long).cuda()\n",
    "        y_val_fold = torch.tensor(y_train[valid_index], dtype=torch.float32).cuda()\n",
    "        t = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        v = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(t, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(v, batch_size=batch_size, shuffle=False)\n",
    "        evaluator = JigsawEvaluator(y_val_target, y_val_identity)\n",
    "    \n",
    "        # model\n",
    "        model = EmbLSTM(embedding_matrix, max_features).cuda()\n",
    "        #model = EmbLSTMGRUCNN(embedding_matrix, max_features).cuda()\n",
    "        criterion = custom_loss\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, amsgrad=False)\n",
    "        #scheduler_cosine = CosineAnnealingLR(optimizer, T_max=4, eta_min=1e-3)\n",
    "        #scheduler = GradualWarmupScheduler(optimizer, multiplier=1.2, total_epoch=2, after_scheduler=scheduler_cosine)\n",
    "    #     scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "    #     scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    \n",
    "        # main loop\n",
    "        best_epoch = -1\n",
    "        best_score = 0.\n",
    "\n",
    "        train_losses, valid_losses = [], []\n",
    "\n",
    "        mb = master_bar(range(epochs))\n",
    "        for epoch in mb:\n",
    "            #scheduler.step() # 2 epoch warmup, after that schedule as scheduler_cosine\n",
    "            #logger.info(f'Epoch {epoch+1} - optimizer: {optimizer.state_dict()[\"param_groups\"]} - scheduler: {scheduler.state_dict()}')\n",
    "            log_dict_train = train_single_epoch(model, train_loader, criterion, optimizer, epoch, mb) # loss, lr\n",
    "            log_dict_valid = evaluate_single_epoch(model, valid_loader, criterion, epoch, evaluator) # loss, score\n",
    "            train_losses.append(log_dict_train['loss'])\n",
    "            valid_losses.append(log_dict_valid['loss'])\n",
    "\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                mb.write(f'Epoch {epoch+1} - avg_train_loss: {log_dict_train[\"loss\"]:.4f}  avg_val_loss: {log_dict_valid[\"loss\"]:.4f}  val_score: {log_dict_valid[\"score\"]:.6f}')\n",
    "                logger.info(f'Epoch {epoch+1} - avg_train_loss: {log_dict_train[\"loss\"]:.4f} avg_val_loss: {log_dict_valid[\"loss\"]:.4f}  val_score: {log_dict_valid[\"score\"]:.6f}')\n",
    "\n",
    "            if log_dict_valid[\"score\"] > best_score:\n",
    "                best_epoch = epoch + 1\n",
    "                best_score = log_dict_valid[\"score\"]\n",
    "                save_checkpoint(f'weight_best_fold_{fold}.pth', model, optimizer, epoch)\n",
    "                if epoch - best_epoch > 3:\n",
    "                    break\n",
    "\n",
    "        # load best\n",
    "        state = torch.load(f'weight_best_fold_{fold}.pth')\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "        # save valid\n",
    "        log_dict = evaluate_single_epoch(model, valid_loader, criterion, 0, evaluator, return_pred=True)\n",
    "        pred_valid = log_dict['pred']\n",
    "        np.save(f'pred_valid_fold_{fold}.npy', pred_valid)\n",
    "\n",
    "        # evaluate\n",
    "        score, subgroup_auc, bpsn_auc, bnsp_auc = evaluator.get_final_metric(pred_valid[:, 0])\n",
    "        subgroup_auc = evaluator._power_mean(subgroup_auc)\n",
    "        bpsn_auc = evaluator._power_mean(bpsn_auc)\n",
    "        bnsp_auc = evaluator._power_mean(bnsp_auc)\n",
    "        overall_auc = evaluator._calculate_overall_auc(pred_valid[:, 0])\n",
    "        logger.info(f'metric: {score:.6f}, overall auc: {overall_auc:.6f}, subgroup_auc: {subgroup_auc:.6f}, bpsn_auc: {bpsn_auc:.6f}, bnsp_auc: {bnsp_auc:.6f}')\n",
    "        \n",
    "        # plot\n",
    "        plot_losses(train_losses, valid_losses, fold=fold)\n",
    "        plot_histogram(y_train[valid_index], pred_valid, n_bins=50, fold=fold)\n",
    "        plot_aucs(log_dict['subgroup_auc'], 'subgroup_auc', fold=fold)\n",
    "        plot_aucs(log_dict['bpsn_auc'], 'bpsn_auc', fold=fold)\n",
    "        plot_aucs(log_dict['bnsp_auc'], 'bnsp_auc', fold=fold)\n",
    "    \n",
    "        del model; gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1764964</th>\n",
       "      <td>6284786</td>\n",
       "      <td>&gt;&gt;&gt; The link was clicked twice\\n\\nIdiots.</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>5.142393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960637</th>\n",
       "      <td>5293418</td>\n",
       "      <td>yes, stupiditing</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.006883</td>\n",
       "      <td>4.481544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179407</th>\n",
       "      <td>5557347</td>\n",
       "      <td>one ILH in top tier? Shows oia is scaid  ILH would play against itself every year.</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>3.956487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919326</th>\n",
       "      <td>5244486</td>\n",
       "      <td>Ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha (choke, gasp) ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha haha ha ha ha ha ha ha ha ha ha ha ha ha (choke, gasp) ha ha ha ha ha ha ha ha ha ha ha ha ha (choke, gasp) ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha haha ha ha ha ha ha ha ha ha ha ha ha ha (choke, gasp) ha ha ha ha.......\\n\\nOntarians can't be THAT stupid to fall for this again</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>3.928247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687700</th>\n",
       "      <td>1082705</td>\n",
       "      <td>Why I said what I said. My imbcil is tightening as we speak.</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>3.888541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613432</th>\n",
       "      <td>6097463</td>\n",
       "      <td>sit it out stupiddd</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>3.804196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613111</th>\n",
       "      <td>992602</td>\n",
       "      <td>For the Trump administration and Trump himself, this was one of those boxes that had to be checked off. They are certainly aware of all the media reports from Canada, and knew they had to get this out of the way. Trump gave the appearance of a man who would rather have been back golfing in Florida. Watching and listening to him read his lines, was like watching paint dry. Success for Trudeau, was not having Trump go ballistic.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023136</td>\n",
       "      <td>3.766352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751271</th>\n",
       "      <td>6268828</td>\n",
       "      <td>Well,I guess that's why they call the boy \"Spook Niggy Nigg\"</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>3.752863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189740</th>\n",
       "      <td>473165</td>\n",
       "      <td>KISS\\n\\nKEEP   IT   SIMPLE   STUPID\\n\\nTalk now about a SuperGrid future, but work on offering our quickest quality export product to Japan now, which is LPG-propane and AK-GTL'\\n\\nSTUB2HUB &gt; Deadhorse to Fairbanks, ASAP with flexpipe only.\\n$500million CAPEX-risk.\\n\\nSiluria turns natural gas into gasoline for $1 per gallon\\nSiluria partners with oil industry giants to make fuels cheaply\\n\\nDo Not focus on selling LNG first in 2016, keep it on the backburner.\\nFocus on micro-GTL  and  micro-GTG  plants located in Fairbanks.\\nTrying to save TAPS in it's current configuration is packing sand down a rathole.\\n\\nhttp://siluria.com/</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.012039</td>\n",
       "      <td>3.538136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536867</th>\n",
       "      <td>6002824</td>\n",
       "      <td>fuckcorygardner.org</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>3.525559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187370</th>\n",
       "      <td>5567324</td>\n",
       "      <td>The buffoonest of all time.</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>3.440610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442596</th>\n",
       "      <td>785850</td>\n",
       "      <td>sanity finally in the usa after 8 years of stupdity</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.014106</td>\n",
       "      <td>3.411772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389623</th>\n",
       "      <td>719603</td>\n",
       "      <td>Parity has worked out so well.  Monsef the Iranian Spy and Freeland the Weeper</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.017235</td>\n",
       "      <td>3.386921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797659</th>\n",
       "      <td>5096861</td>\n",
       "      <td>I'll bet that would be just tickety boo.</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>3.366538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423409</th>\n",
       "      <td>761908</td>\n",
       "      <td>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n                                                                                                                                                                        (1)  \"Sorry, I'm on sabbatical from having to contemplate +\\nreact to despicable beings or things that I cannot remedy.\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\n                                                                                                                                                                               \\n                                                               \\n                  \\n (2)  \"No, my goodness, indeed he's not.  But, hey, I can\\nsee he's meretricious to many.  Y'all have a nice day....\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>3.229897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343898</th>\n",
       "      <td>5757957</td>\n",
       "      <td>conservative?\\nno\\nlunatic</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>3.194242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683976</th>\n",
       "      <td>1078310</td>\n",
       "      <td>So I have 2 down votes. Next time you don't feel like paying contract obligations see what happens.\\nAs for the curent mess - it is a sunk cost and our only option is to stop \"green energy\" ( solar &amp; wind ) and hook up to Quebec hydro, and/or go nuclear, ..... Either way we are s c r e w e d .......</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.021723</td>\n",
       "      <td>3.067917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762096</th>\n",
       "      <td>5052932</td>\n",
       "      <td>Sorry larned we dont have cash to keep blowing on so called art. Rather have things that payoff, like the wall to keep out the scumbums.</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.026460</td>\n",
       "      <td>3.031225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625504</th>\n",
       "      <td>6113076</td>\n",
       "      <td>What a  Dyck!</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.015777</td>\n",
       "      <td>2.909222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885633</th>\n",
       "      <td>5204081</td>\n",
       "      <td>Des young hommie dindunuffin he jes livin de life O his rap star hero's. Now des young hommie got es street creeds an can pursue dat dream O bein de rap star O es dreams</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>2.901956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355487</th>\n",
       "      <td>678377</td>\n",
       "      <td>will poeple with crimanle back grounds get housing......the salem housing athority dose not rent to poeple with crimenle back grounds.....</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.016207</td>\n",
       "      <td>2.890506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672751</th>\n",
       "      <td>1064544</td>\n",
       "      <td>Appreciate the player updates.  Keep them coming.</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009551</td>\n",
       "      <td>2.794513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360265</th>\n",
       "      <td>684410</td>\n",
       "      <td>Piffle!</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>2.783993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855926</th>\n",
       "      <td>5167691</td>\n",
       "      <td>HAHAHAHAHA! Loooooooser!!!!</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063126</td>\n",
       "      <td>2.762623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541961</th>\n",
       "      <td>6009214</td>\n",
       "      <td>I am going to assume that you are a young person unaware of American history, Google Walter Cronkite and President Lyndon B Johnson.</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.010229</td>\n",
       "      <td>2.753653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634683</th>\n",
       "      <td>1018205</td>\n",
       "      <td>Now if Canada would only be so kind as to take the 15 million Mexicans of our hands.  \\nAmerica would be very thankful.</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.037010</td>\n",
       "      <td>2.753433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170161</th>\n",
       "      <td>450117</td>\n",
       "      <td>Way to like yourself loserboy</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>2.711990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673702</th>\n",
       "      <td>6173720</td>\n",
       "      <td>Is there at way they could \"fire\" each other ?!\\nSimultaneously  .-)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067050</td>\n",
       "      <td>2.702321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380377</th>\n",
       "      <td>5804934</td>\n",
       "      <td>Not only irrelevant but inapt. Delete.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067435</td>\n",
       "      <td>2.696586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796974</th>\n",
       "      <td>6323498</td>\n",
       "      <td>But, is she gonna strip?</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.039826</td>\n",
       "      <td>2.692796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    ...      logloss\n",
       "1764964  6284786    ...     5.142393\n",
       "960637   5293418    ...     4.481544\n",
       "1179407  5557347    ...     3.956487\n",
       "919326   5244486    ...     3.928247\n",
       "687700   1082705    ...     3.888541\n",
       "1613432  6097463    ...     3.804196\n",
       "613111   992602     ...     3.766352\n",
       "1751271  6268828    ...     3.752863\n",
       "189740   473165     ...     3.538136\n",
       "1536867  6002824    ...     3.525559\n",
       "1187370  5567324    ...     3.440610\n",
       "442596   785850     ...     3.411772\n",
       "389623   719603     ...     3.386921\n",
       "797659   5096861    ...     3.366538\n",
       "423409   761908     ...     3.229897\n",
       "1343898  5757957    ...     3.194242\n",
       "683976   1078310    ...     3.067917\n",
       "762096   5052932    ...     3.031225\n",
       "1625504  6113076    ...     2.909222\n",
       "885633   5204081    ...     2.901956\n",
       "355487   678377     ...     2.890506\n",
       "672751   1064544    ...     2.794513\n",
       "360265   684410     ...     2.783993\n",
       "855926   5167691    ...     2.762623\n",
       "1541961  6009214    ...     2.753653\n",
       "634683   1018205    ...     2.753433\n",
       "170161   450117     ...     2.711990\n",
       "1673702  6173720    ...     2.702321\n",
       "1380377  5804934    ...     2.696586\n",
       "1796974  6323498    ...     2.692796\n",
       "\n",
       "[30 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for fold, (train_index, valid_index) in enumerate(cv.split(range(len(train)), np.where(train[target]>0.5, 1, 0))):\n",
    "    if fold==n_fold:\n",
    "        valid = train.loc[valid_index]\n",
    "        valid['pred'] = np.load(f'pred_valid_fold_{fold}.npy')[:, 0]\n",
    "        valid['logloss'] = log_loss(valid[target].values, valid['pred'].values)\n",
    "        break\n",
    "\n",
    "valid.sort_values('logloss', ascending=False).loc[:, ['id', 'comment_text', 'target', 'pred', 'logloss']].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(y_pred):\n",
    "    sub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
    "    sub['prediction'] = y_pred[:, 0]\n",
    "    sub.to_csv('submission.csv', index=False)\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_loader = prepare_data_loader(X_test, test_lengths, shuffle=False)\n",
    "\n",
    "pred_tests = []\n",
    "weights = sorted(glob.glob('weight_best_fold_*.pth'))\n",
    "for fold, path in enumerate(weights):\n",
    "    model = EmbLSTM(embedding_matrix, max_features).cuda()\n",
    "    state = torch.load(path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    \n",
    "    pred_test = predict_model(model, test_loader)\n",
    "    np.save(f'pred_test_fold_{fold}.npy', pred_test)\n",
    "    pred_tests.append(pred_test)\n",
    "    \n",
    "    del model; gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "pred_tests = np.mean(pred_tests, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub = submission(pred_tests)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.clf()\n",
    "plt.hist(pred_tests[:, 0])\n",
    "plt.title('test histogram')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
